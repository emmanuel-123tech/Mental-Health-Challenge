{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10758994,"sourceType":"datasetVersion","datasetId":6673667}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import Dataset\n# Install necessary libraries for transformers and NLP\n!pip install -q transformers datasets torch accelerate\n!pip install transformers datasets torch accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:59:40.686041Z","iopub.execute_input":"2025-02-15T21:59:40.686425Z","iopub.status.idle":"2025-02-15T21:59:40.692342Z","shell.execute_reply.started":"2025-02-15T21:59:40.686396Z","shell.execute_reply":"2025-02-15T21:59:40.691425Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\n\n# Load the provided datasets\ntrain_path = \"/kaggle/input/datascienee/train (15).csv\"\ntest_path = \"/kaggle/input/datascienee/test (15).csv\"\n#submission_path = \"/mnt/data/SampleSubmission (13).csv\"\n\n# Read the datasets\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\n#submission_df = pd.read_csv(submission_path)\n\n# Display the first few rows of each dataset to understand the structure\ntrain_df.head(), test_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:59:41.547663Z","iopub.execute_input":"2025-02-15T21:59:41.547994Z","iopub.status.idle":"2025-02-15T21:59:42.766855Z","shell.execute_reply.started":"2025-02-15T21:59:41.547966Z","shell.execute_reply":"2025-02-15T21:59:42.766097Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(      id                                          title  \\\n 0  11098                            Getting of the weed   \n 1    116  Seeking advice on how to face a hospital stay   \n 2   7189              Trying to re-enter the work place   \n 3   4350                       Family history of cancer   \n 4   9749     Inappropriate relationship with therapist.   \n \n                                              content  \\\n 0  [Post removed at request of member]\\nHi, welco...   \n 1  Hi NMTB,\\nThanks for your post. \\nI think a lo...   \n 2  Hello, \\nIâ€™m Cas and for a fair while now I ha...   \n 3  Hey everyone.\\nI've just discovered that anoth...   \n 4  Hi everyone\\nI guess the title says it all rea...   \n \n                             target  \n 0  suicidal-thoughts-and-self-harm  \n 1                          anxiety  \n 2                          anxiety  \n 3                          anxiety  \n 4                       depression  ,\n       id                                              title  \\\n 0   3639                                          Tailgated   \n 1  21493  I am a model and because of mean things my mot...   \n 2  21215     On-and-off nothingness. I donâ€™t know any more.   \n 3  13466                              Feeling really scared   \n 4  14084                  Looking for people who understand   \n \n                                              content  \n 0  Hello. I was badly tailgated on a busy highway...  \n 1  Morning Scapegoated.\\nOk. Scape goated is defi...  \n 2  Hi. \\nAs the title says, I really just donâ€™t k...  \n 3  Hello I am new to all this online forums and s...  \n 4  Hey all.. I never thought I'd be looking for h...  )"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Fill missing values\ntrain_df[\"content\"].fillna(train_df[\"title\"], inplace=True)\ntest_df[\"content\"].fillna(test_df[\"title\"], inplace=True)\n\n# Encode target labels\nlabel_encoder = LabelEncoder()\ntrain_df[\"label\"] = label_encoder.fit_transform(train_df[\"target\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T22:00:10.000879Z","iopub.execute_input":"2025-02-15T22:00:10.001292Z","iopub.status.idle":"2025-02-15T22:00:10.051091Z","shell.execute_reply.started":"2025-02-15T22:00:10.001258Z","shell.execute_reply":"2025-02-15T22:00:10.050215Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-12-99b5ab0f5ffc>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df[\"content\"].fillna(train_df[\"title\"], inplace=True)\n<ipython-input-12-99b5ab0f5ffc>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test_df[\"content\"].fillna(test_df[\"title\"], inplace=True)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Train-Validation Split (80% train, 20% validation)\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_df[\"content\"], train_df[\"label\"], test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T22:00:14.950126Z","iopub.execute_input":"2025-02-15T22:00:14.950439Z","iopub.status.idle":"2025-02-15T22:00:14.959394Z","shell.execute_reply.started":"2025-02-15T22:00:14.950415Z","shell.execute_reply":"2025-02-15T22:00:14.958627Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\n# Tokenization function\ndef tokenize_data(examples):\n    return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=256)\n\n# Convert to Hugging Face dataset format\ntrain_dataset = Dataset.from_dict({\"content\": train_texts.tolist(), \"label\": train_labels.tolist()})\nval_dataset = Dataset.from_dict({\"content\": val_texts.tolist(), \"label\": val_labels.tolist()})\n\n# Apply tokenization\ntrain_dataset = train_dataset.map(tokenize_data, batched=True)\nval_dataset = val_dataset.map(tokenize_data, batched=True)\n\n# Load pre-trained RoBERTa model\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_))\n\n# Define Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,  # Include validation dataset\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T22:01:07.026614Z","iopub.execute_input":"2025-02-15T22:01:07.026902Z","iopub.status.idle":"2025-02-15T22:03:33.348386Z","shell.execute_reply.started":"2025-02-15T22:01:07.026882Z","shell.execute_reply":"2025-02-15T22:03:33.347501Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17720 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c22442ed9aff40e2a5be8dbc9269e132"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4431 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23e59fd600e4882bdd10896483138d8"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T22:03:48.319825Z","iopub.execute_input":"2025-02-15T22:03:48.320125Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Save model\nmodel.save_pretrained(\"roberta_model\")\ntokenizer.save_pretrained(\"roberta_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict on test set\ntest_encodings = tokenizer(list(test_df[\"content\"]), truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\ntest_inputs = {key: value for key, value in test_encodings.items()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get predictions\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**test_inputs)\npredictions = torch.argmax(outputs.logits, dim=-1).numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert predictions back to labels\ntest_df[\"target\"] = label_encoder.inverse_transform(predictions)\n\n# Save submission\ntest_df[[\"id\", \"target\"]].to_csv(\"enova_1st.csv\", index=False)\nprint(\"Submission saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
